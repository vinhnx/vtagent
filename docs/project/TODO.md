https://github.com/openai/codex/blob/main/codex-rs/core/gpt_5_codex_prompt.md

--

https://github.com/openai/codex/blob/main/codex-rs/core/prompt.md

--

The terminal UI has also been upgraded: tool calls and diffs are better formatted and easier to follow. Approval modes are simplified to three levels: read-only with explicit approvals, auto with full workspace access but requiring approvals outside the workspace, and full access with the ability to read files anywhere and run commands with network access.

--

https://openai.com/index/introducing-upgrades-to-codex/

upgrade codex

--

integrate https://deepwiki.com/pawurb/hotpath profiler and instrument then fix blocking code.
A simple Rust profiler that shows exactly where your code spends time

---

--

https://deepwiki.com/crate-ci/cargo-release

--

9:26:28 ❯ codex

╭────────────────────────────────────────────────────────╮
│ >\_ OpenAI Codex (v0.36.0) │
│ │
│ model: gpt-5-codex /model to change │
│ directory: ~/Developer/learn-by-doing/vtagent │
╰────────────────────────────────────────────────────────╯

To get started, describe a task or try one of these commands:

/init - create an AGENTS.md file with instructions for Codex
/status - show current session configuration
/approvals - choose what Codex can do without approval
/model - choose what model and reasoning effort to use

> Model changed to gpt-5-codex

▌ Find and fix a bug in @filename

⏎ send ⇧⏎ newline ⌃T transcript ⌃C quit

---

Implement llm vtagent response token streaming for real-time response generation. Stream output in plain text during the agent's response. for tools call doesn't need streaming. make sure the streaming works with chat command. write tests for streaming functionality. update docs if necessary. update system prompt if needed. make sure streaming works with all models.

---

check docs/guides/codex-cloud-setup.md
and setup codex cloud environment for vtagent
https://developers.openai.com/codex/cloud/environments
