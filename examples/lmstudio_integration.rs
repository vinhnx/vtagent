//! Minimal example showing LMStudio integration\n//! This is a standalone example that works independently of the broken VTAgent codebase\n\nuse std::error::Error;\nuse serde::{Deserialize, Serialize};\nuse serde_json::Value;\n\n/// Simple struct to represent an LMStudio client\npub struct LMStudioClient {\n    base_url: String,\n    client: reqwest::Client,\n}\n\n/// Request structure for completions\n#[derive(Serialize)]\nstruct CompletionRequest {\n    model: String,\n    messages: Vec<Message>,\n    temperature: Option<f32>,\n    max_tokens: Option<u32>,\n}\n\n/// Message structure\n#[derive(Serialize, Clone)]\nstruct Message {\n    role: String,\n    content: String,\n}\n\n/// Response structure\n#[derive(Deserialize)]\nstruct CompletionResponse {\n    choices: Vec<Choice>,\n}\n\n#[derive(Deserialize)]\nstruct Choice {\n    message: Message,\n}\n\nimpl LMStudioClient {\n    /// Create a new LMStudio client\n    pub fn new() -> Result<Self, Box<dyn Error>> {\n        Ok(Self {\n            base_url: \"http://localhost:1234/v1\".to_string(),\n            client: reqwest::Client::new(),\n        })\n    }\n\n    /// Send a completion request to LMStudio\n    pub async fn complete(\n        &self,\n        model: &str,\n        prompt: &str,\n    ) -> Result<String, Box<dyn Error>> {\n        let request = CompletionRequest {\n            model: model.to_string(),\n            messages: vec![\n                Message {\n                    role: \"user\".to_string(),\n                    content: prompt.to_string(),\n                }\n            ],\n            temperature: Some(0.7),\n            max_tokens: Some(500),\n        };\n\n        let url = format!(\"{}/chat/completions\", self.base_url);\n        \n        let response = self.client\n            .post(&url)\n            .header(\"Content-Type\", \"application/json\")\n            .json(&request)\n            .send()\n            .await?;\n\n        if response.status().is_success() {\n            let completion_response: CompletionResponse = response.json().await?;\n            if let Some(choice) = completion_response.choices.first() {\n                Ok(choice.message.content.clone())\n            } else {\n                Err(\"No completion choices returned\".into())\n            }\n        } else {\n            let status = response.status();\n            let error_text = response.text().await?;\n            Err(format!(\"HTTP {}: {}\", status, error_text).into())\n        }\n    }\n\n    /// List available models\n    pub async fn list_models(&self) -> Result<Value, Box<dyn Error>> {\n        let url = format!(\"{}/models\", self.base_url);\n        let response = self.client\n            .get(&url)\n            .send()\n            .await?;\n\n        if response.status().is_success() {\n            let models: Value = response.json().await?;\n            Ok(models)\n        } else {\n            let status = response.status();\n            let error_text = response.text().await?;\n            Err(format!(\"HTTP {}: {}\", status, error_text).into())\n        }\n    }\n}\n\n/// Test LMStudio connectivity\nasync fn test_lmstudio_connectivity() -> Result<(), Box<dyn Error>> {\n    println!(\"🔍 Testing LMStudio connectivity...\");\n    \n    match LMStudioClient::new() {\n        Ok(client) => {\n            println!(\"✅ LMStudio client created successfully\");\n            \n            // Test models endpoint\n            match client.list_models().await {\n                Ok(models) => {\n                    println!(\"✅ Successfully connected to LMStudio!\");\n                    println!(\"📄 Available models:\");\n                    \n                    if let Some(data) = models.get(\"data\").and_then(|d| d.as_array()) {\n                        for model in data.iter().take(3) {\n                            if let Some(id) = model.get(\"id\").and_then(|i| i.as_str()) {\n                                println!(\"   - {}\", id);\n                            }\n                        }\n                        if data.len() > 3 {\n                            println!(\"   ... and {} more\", data.len() - 3);\n                        }\n                    }\n                }\n                Err(e) => {\n                    println!(\"❌ Failed to list models: {}\", e);\n                    println!(\"💡 Make sure LMStudio is running and the server is started\");\n                    return Ok(());\n                }\n            }\n            \n            // Test a simple completion (this will fail if no model is loaded)\n            println!(\"\\n🧪 Testing completion with model 'qwen3-4b-2507'...\");\n            match client.complete(\"qwen3-4b-2507\", \"Say hello world\").await {\n                Ok(response) => {\n                    println!(\"✅ Completion successful!\");\n                    println!(\"📄 Response: {}\", response);\n                }\n                Err(e) => {\n                    println!(\"⚠️  Completion test failed (this is OK if no model is loaded): {}\", e);\n                    println!(\"💡 This is expected if you haven't loaded a model in LMStudio yet\");\n                }\n            }\n        }\n        Err(e) => {\n            println!(\"❌ Failed to create LMStudio client: {}\", e);\n            return Ok(());\n        }\n    }\n    \n    println!(\"\\n🎉 LMStudio test completed!\");\n    println!(\"📝 Next steps:\");\n    println!(\"   1. Make sure LMStudio is running\");\n    println!(\"   2. Load a model in LMStudio\");\n    println!(\"   3. Start the server in LMStudio\");\n    println!(\"   4. Run this test again\");\n    \n    Ok(())\n}\n\n/// Simple chat interface\nasync fn chat_interface() -> Result<(), Box<dyn Error>> {\n    let client = LMStudioClient::new()?;\n    let model = \"qwen3-4b-2507\"; // Default model, change as needed\n    \n    println!(\"💬 LMStudio Chat Interface\");\n    println!(\"📝 Enter your messages (type 'quit' to exit)\");\n    println!(\"🤖 Using model: {}\", model);\n    println!();\n    \n    loop {\n        use std::io::{self, Write};\n        \n        print!(\"You: \");\n        io::stdout().flush()?;\n        \n        let mut input = String::new();\n        io::stdin().read_line(&mut input)?;\n        let input = input.trim();\n        \n        if input.eq_ignore_ascii_case(\"quit\") || input.eq_ignore_ascii_case(\"exit\") {\n            println!(\"👋 Goodbye!\");\n            break;\n        }\n        \n        if input.is_empty() {\n            continue;\n        }\n        \n        match client.complete(model, input).await {\n            Ok(response) => {\n                println!(\"🤖 Assistant: {}\", response);\n            }\n            Err(e) => {\n                println!(\"❌ Error: {}\", e);\n                println!(\"💡 Make sure LMStudio is running with a model loaded\");\n            }\n        }\n        \n        println!();\n    }\n    \n    Ok(())\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn Error>> {\n    let args: Vec<String> = std::env::args().collect();\n    \n    if args.len() > 1 && args[1] == \"test\" {\n        test_lmstudio_connectivity().await?;\n    } else if args.len() > 1 && args[1] == \"chat\" {\n        chat_interface().await?;\n    } else {\n        println!(\"<LMStudio Integration Example>\");\n        println!();\n        println!(\"This is a minimal standalone example showing how to use LMStudio.\");\n        println!();\n        println!(\"Usage:\");\n        println!(\"  cargo run --example lmstudio_integration test   # Test LMStudio connectivity\");\n        println!(\"  cargo run --example lmstudio_integration chat   # Simple chat interface\");\n        println!();\n        println!(\"Make sure LMStudio is running with a model loaded before testing!\");\n    }\n    \n    Ok(())\n}\n\n// Add this to your Cargo.toml to run this example:\n// \n// [[example]]\n// name = \"lmstudio_integration\"\n// path = \"examples/lmstudio_integration.rs\"
